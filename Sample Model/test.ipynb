{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 11:35:51.746683: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-02 11:35:51.746708: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-02 11:35:51.747732: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-02 11:35:51.753570: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-02 11:35:52.411848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(inputs, filter,block_num,reduce=False):\n",
    "    res = inputs\n",
    "    x = keras.layers.Conv2D(filter,(3, 3), strides = 2 if reduce else 1,activation = 'relu', padding = 'same',name = f'conv_{block_num}_1')(res)\n",
    "    x = keras.layers.Conv2D(filter,(3, 3), activation = 'relu', padding = 'same',name = f'conv_{block_num}_2')(x)\n",
    "    if reduce == True:\n",
    "        res = keras.layers.Conv2D(filter,(1, 1), strides = 2, activation = 'relu', padding = 'same',name = f'conv_skip_{block_num-1}')(res)\n",
    "    res = keras.layers.Add(name = f'end_res_bock_{2*block_num-1}')([x,res])\n",
    "    x = keras.layers.Conv2D(filter,(3, 3), activation = 'relu', padding = 'same',name = f'conv_{block_num}_3')(res)\n",
    "    x = keras.layers.Conv2D(filter,(3, 3), activation = 'relu', padding = 'same',name = f'conv_{block_num}_4')(x)\n",
    "    res = keras.layers.Add(name = f'end_res_bock_{2*block_num}')([x,res])\n",
    "    return res\n",
    "\n",
    "def resnet18(num_outputs=1):\n",
    "    # Inputs\n",
    "    inputs = keras.layers.Input(shape=(224,224,3),name = 'input')\n",
    "    x = keras.layers.Conv2D(64,(7, 7),strides = 2, activation = 'relu', padding = 'same',name = 'conv2D')(inputs)\n",
    "    x = keras.layers.MaxPooling2D(pool_size = (3,3),strides = 2 ,padding = 'same',name = 'max_pool_1')(x)\n",
    "    # Block 1\n",
    "    x = residual_block(x,64,1,False)\n",
    "    # Block 2\n",
    "    x = residual_block(x,128,2,True)\n",
    "    # Block 3\n",
    "    x = residual_block(x,256,3,True)\n",
    "    # Block 4\n",
    "    x = residual_block(x,512,4,True)\n",
    "    # Ouputs\n",
    "    x = keras.layers.GlobalAveragePooling2D(name = 'global_pooling')(x)\n",
    "    if num_outputs==1:\n",
    "        outputs = keras.layers.Dense(num_outputs,activation = 'sigmoid',name = 'outputs_sigmoid')(x)\n",
    "    else:\n",
    "        outputs = keras.layers.Dense(num_outputs,activation = 'softmax',name = 'outputs_softmax')(x)\n",
    "    model = keras.Model(inputs = inputs, outputs = outputs, name = 'resnet18')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24959 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "train_gen = img_gen.flow_from_directory('./data',class_mode=\"binary\",target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 11:36:01.532910: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 11:36:01.568483: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 11:36:01.568685: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 11:36:01.569628: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 11:36:01.569785: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 11:36:01.569923: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 11:36:01.645616: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 11:36:01.645789: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 11:36:01.645933: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 11:36:01.646034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6185 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(1)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "train_acc_metric = keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 11:36:06.717168: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-01-02 11:36:06.800978: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-02 11:36:07.011989: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-02 11:36:09.932329: I external/local_xla/xla/service/service.cc:168] XLA service 0x1cb9d550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-02 11:36:09.932348: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2070, Compute Capability 7.5\n",
      "2024-01-02 11:36:09.936522: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1704170170.025309  510357 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fa9e40adab0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fa9e40adab0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training loss (for one batch) at step 0: 0.6939\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6813\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6899\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6833\n",
      "Seen so far: 19232 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/tan/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.6863\n",
      "Seen so far: 32 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_gen):\n",
    "        y_batch_train = y_batch_train.reshape(-1,1)\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n",
    "        if step==len(train_gen)-1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - 30s 38ms/step - loss: 0.2175 - acc: 0.9075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2175300270318985, 0.9074882864952087]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 14:24:06.696502: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2023-12-26 14:24:06.786929: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-26 14:24:07.317086: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/780 [====================>.........] - ETA: 8s - loss: 0.6963 - binary_accuracy: 0.5003"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/tan/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - 32s 39ms/step - loss: 0.6963 - binary_accuracy: 0.5004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6962601542472839, 0.5004206895828247]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss= loss_fn,metrics=keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 13:53:00.959289: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2023-12-26 13:53:01.049756: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-26 13:53:01.591411: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-26 13:53:02.422421: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd479a47610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-26 13:53:02.422443: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2070, Compute Capability 7.5\n",
      "2023-12-26 13:53:02.427321: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1703573582.507264  198095 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368/780 [=============>................] - ETA: 25s - loss: 0.7403 - binary_accuracy: 0.5223"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/tan/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - 54s 60ms/step - loss: 0.7137 - binary_accuracy: 0.5302\n",
      "Epoch 2/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.6815 - binary_accuracy: 0.5654\n",
      "Epoch 3/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.6693 - binary_accuracy: 0.5898\n",
      "Epoch 4/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.6419 - binary_accuracy: 0.6279\n",
      "Epoch 5/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.6030 - binary_accuracy: 0.6736\n",
      "Epoch 6/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.5503 - binary_accuracy: 0.7209\n",
      "Epoch 7/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.4902 - binary_accuracy: 0.7636\n",
      "Epoch 8/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.4441 - binary_accuracy: 0.7904\n",
      "Epoch 9/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.3980 - binary_accuracy: 0.8204\n",
      "Epoch 10/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.3506 - binary_accuracy: 0.8457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd57415bdc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_gen,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speedup with tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value,grads\n",
    "@tf.function\n",
    "def optimize_model(grads):\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.5041\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.3931\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.3361\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.4429\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.8048\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.2352\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6619\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.2736\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.3332\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.8372\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_gen):\n",
    "        y_batch_train = y_batch_train.reshape(-1,1)\n",
    "        loss_value,grads = train_step(x_batch_train,y_batch_train)\n",
    "        optimize_model(grads)\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n",
    "        if step==len(train_gen)-1:\n",
    "            break\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.5662\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6559\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6046\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.4874\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.7032\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.5174\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.5484\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.5752\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.4768\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.7483\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.4044\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.4493\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.3160\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.5423\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.7782\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.5421\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.3301\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.4634\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6127\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.7950\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.4242\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.3520\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.3474\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.5257\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.8139\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.2386\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.3900\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.3156\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.3988\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.8413\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.4020\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.2511\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.1498\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.5650\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.8554\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_gen):\n",
    "        y_batch_train = y_batch_train.reshape(-1,1)\n",
    "        loss_value = train_step(x_batch_train,y_batch_train)\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n",
    "        if step==len(train_gen)-1:\n",
    "            break\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
