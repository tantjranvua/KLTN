{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 12:11:34.626205: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-20 12:11:34.626239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-20 12:11:34.627342: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-20 12:11:34.633361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-20 12:11:35.302653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-20 12:11:36.008725: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-20 12:11:36.043681: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-20 12:11:36.043868: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-20 12:11:36.045559: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-20 12:11:36.045712: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-20 12:11:36.045845: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-20 12:11:36.136610: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-20 12:11:36.136792: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-20 12:11:36.136891: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-01-20 12:11:36.136939: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-20 12:11:36.137037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6429 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import modelbuild\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = modelbuild.buildmodel()\n",
    "model2 = modelbuild.buildmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24959 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "data_flow = img_gen.flow_from_directory('./data',target_size=(224,224),class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 16:26:45.210933: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-01-17 16:26:45.308425: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-17 16:26:45.900541: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60/780 [=>............................] - ETA: 30s - loss: 0.7004 - binary_accuracy: 0.4891"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/tan/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - 36s 44ms/step - loss: 0.6980 - binary_accuracy: 0.4996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6980067491531372, 0.4995793104171753]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile()\n",
    "model1.model.evaluate(data_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - 32s 40ms/step - loss: 0.6980 - binary_accuracy: 0.4996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6980071663856506, 0.4995793104171753]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile()\n",
    "model2.model.evaluate(data_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/tan/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model1.model.save('model.h5')\n",
    "model2.model = keras.models.load_model('model.h5')\n",
    "# model2.model.evaluate(data_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_flow.next()\n",
    "x,y =data\n",
    "y = y.reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 02:19:28.218217: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-01-19 02:19:28.316854: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-19 02:19:28.865141: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-19 02:19:30.097980: W external/local_tsl/tsl/framework/bfc_allocator.cc:366] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "2024-01-19 02:19:32.260209: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fa36b0800c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-19 02:19:32.260232: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2070, Compute Capability 7.5\n",
      "2024-01-19 02:19:32.264230: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1705605572.361965  969039 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 0.7065\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6877\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6927\n",
      "Seen so far: 12832 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/tan/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 600: 0.6903\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 81.55457854270935\n",
      "780/780 [==============================] - 29s 37ms/step - loss: 0.6837 - binary_accuracy: 0.5518\n",
      "780/780 [==============================] - 31s 39ms/step - loss: 0.6837 - binary_accuracy: 0.5518\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.7671\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6645\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6926\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.7023\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 72.83371424674988\n",
      "780/780 [==============================] - 30s 39ms/step - loss: 0.6828 - binary_accuracy: 0.5582\n",
      "780/780 [==============================] - 31s 39ms/step - loss: 0.6828 - binary_accuracy: 0.5582\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.7249\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6841\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6267\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6926\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 71.17818856239319\n",
      "780/780 [==============================] - 30s 38ms/step - loss: 0.6833 - binary_accuracy: 0.5523\n",
      "780/780 [==============================] - 30s 39ms/step - loss: 0.6833 - binary_accuracy: 0.5523\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.6800\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.7402\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6505\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.7082\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 72.29081988334656\n",
      "780/780 [==============================] - 32s 40ms/step - loss: 0.6747 - binary_accuracy: 0.5805\n",
      "780/780 [==============================] - 31s 39ms/step - loss: 0.6747 - binary_accuracy: 0.5805\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.6907\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6338\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6981\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.7033\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 71.69013142585754\n",
      "780/780 [==============================] - 31s 40ms/step - loss: 0.6602 - binary_accuracy: 0.6099\n",
      "780/780 [==============================] - 31s 40ms/step - loss: 0.6602 - binary_accuracy: 0.6099\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.6378\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.5954\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6775\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6564\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 72.06114339828491\n",
      "780/780 [==============================] - 32s 41ms/step - loss: 0.6318 - binary_accuracy: 0.6539\n",
      "780/780 [==============================] - 31s 40ms/step - loss: 0.6318 - binary_accuracy: 0.6539\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.6542\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6866\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6239\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.5871\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 71.56603074073792\n",
      "780/780 [==============================] - 30s 39ms/step - loss: 0.5666 - binary_accuracy: 0.7074\n",
      "780/780 [==============================] - 31s 39ms/step - loss: 0.5666 - binary_accuracy: 0.7074\n",
      "Total time: 944.2475452423096\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = 7\n",
    "start_train_time = time.time() \n",
    "for epoch in range(epochs):\n",
    "    start_epoch = time.time()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(data_flow):\n",
    "        y_batch_train = y_batch_train.reshape(-1,1)\n",
    "        # print(step, x_batch_train.shape, y_batch_train.shape)\n",
    "        loss_value,grads = model1.train_step(x_batch_train,y_batch_train)\n",
    "        model2.optimize_model(grads)\n",
    "        # model1.model = model2.model\n",
    "        if (step %2 != 0):\n",
    "            model1.model.set_weights(model2.model.get_weights())\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n",
    "        if step==len(data_flow)-1:\n",
    "            # print(model1.model.evaluate(data_flow))\n",
    "            # print(model2.model.evaluate(data_flow))\n",
    "            break\n",
    "    # Display metrics at the end of each epoch.\n",
    "    # train_acc = model1.metrics.result()\n",
    "    # print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "    print(\"Training time over epoch:\", time.time()-start_epoch)\n",
    "    model1.model.evaluate(data_flow)\n",
    "    model2.model.evaluate(data_flow)\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    model1.metrics.reset_states()\n",
    "print(\"Total time:\", time.time()-start_train_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 23:46:59.002623: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-01-18 23:46:59.099963: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-18 23:46:59.636345: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-18 23:47:00.887361: W external/local_tsl/tsl/framework/bfc_allocator.cc:366] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "2024-01-18 23:47:03.059474: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd228a63050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-18 23:47:03.059497: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2070, Compute Capability 7.5\n",
      "2024-01-18 23:47:03.063526: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1705596423.151332  943062 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 0.7048\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6919\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.7001\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6878\n",
      "Seen so far: 19232 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/tan/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time over epoch: 112.27105522155762\n",
      "780/780 [==============================] - 31s 39ms/step - loss: 0.6927 - binary_accuracy: 0.5331\n",
      "780/780 [==============================] - 31s 39ms/step - loss: 0.6927 - binary_accuracy: 0.5331\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.6914\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.7096\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.7006\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6925\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 98.19841504096985\n",
      "780/780 [==============================] - 31s 40ms/step - loss: 0.6911 - binary_accuracy: 0.5626\n",
      "780/780 [==============================] - 31s 40ms/step - loss: 0.6911 - binary_accuracy: 0.5626\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.6925\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6688\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6747\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6801\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 98.0162079334259\n",
      "780/780 [==============================] - 30s 38ms/step - loss: 0.6727 - binary_accuracy: 0.5793\n",
      "780/780 [==============================] - 31s 39ms/step - loss: 0.6727 - binary_accuracy: 0.5793\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.6558\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6405\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.5914\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6339\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 98.33132123947144\n",
      "780/780 [==============================] - 30s 39ms/step - loss: 0.6523 - binary_accuracy: 0.6325\n",
      "780/780 [==============================] - 30s 38ms/step - loss: 0.6523 - binary_accuracy: 0.6325\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.6184\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6654\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6709\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6101\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 97.81051087379456\n",
      "780/780 [==============================] - 31s 40ms/step - loss: 0.5982 - binary_accuracy: 0.6836\n",
      "780/780 [==============================] - 30s 38ms/step - loss: 0.5982 - binary_accuracy: 0.6836\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.5309\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6323\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6240\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.4452\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 98.10817956924438\n",
      "780/780 [==============================] - 31s 39ms/step - loss: 0.5359 - binary_accuracy: 0.7316\n",
      "780/780 [==============================] - 30s 38ms/step - loss: 0.5359 - binary_accuracy: 0.7316\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.5451\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6486\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.4005\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.4677\n",
      "Seen so far: 19232 samples\n",
      "Training time over epoch: 96.44183659553528\n",
      "780/780 [==============================] - 31s 39ms/step - loss: 0.4984 - binary_accuracy: 0.7577\n",
      "780/780 [==============================] - 30s 39ms/step - loss: 0.4984 - binary_accuracy: 0.7577\n",
      "Total time: 1126.1688990592957\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = 7\n",
    "start_train_time = time.time() \n",
    "for epoch in range(epochs):\n",
    "    start_epoch = time.time()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(data_flow):\n",
    "        y_batch_train = y_batch_train.reshape(-1,1)\n",
    "        # print(step, x_batch_train.shape, y_batch_train.shape)\n",
    "        loss_value,grads = model1.train_step(x_batch_train,y_batch_train)\n",
    "        model2.optimize_model(grads)\n",
    "        # model1.model = model2.model\n",
    "        # if step %2:\n",
    "        model1.model.set_weights(model2.model.get_weights())\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n",
    "        if step==len(data_flow)-1:\n",
    "            # print(model1.model.evaluate(data_flow))\n",
    "            # print(model2.model.evaluate(data_flow))\n",
    "            break\n",
    "    # Display metrics at the end of each epoch.\n",
    "    # train_acc = model1.metrics.result()\n",
    "    # print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "    print(\"Training time over epoch:\", time.time()-start_epoch)\n",
    "    model1.model.evaluate(data_flow)\n",
    "    model2.model.evaluate(data_flow)\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    model1.metrics.reset_states()\n",
    "print(\"Total time:\", time.time()-start_train_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelbuild.buildmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 12:11:49.215634: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-01-20 12:11:49.309249: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-20 12:11:49.851937: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423/780 [===============>..............] - ETA: 14s - loss: 0.6926 - binary_accuracy: 0.4972"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/tan/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - 34s 42ms/step - loss: 0.6923 - binary_accuracy: 0.5004\n",
      "[0.6922769546508789, 0.5004206895828247]\n",
      "780/780 [==============================] - 33s 42ms/step - loss: 0.6855 - binary_accuracy: 0.5615\n",
      "[0.6854561567306519, 0.5614808201789856]\n",
      "308/780 [==========>...................] - ETA: 18s - loss: 0.6812 - binary_accuracy: 0.5686"
     ]
    }
   ],
   "source": [
    "for i in range(1,16):\n",
    "    model.model = keras.models.load_model('model1'+str(i)+'.h5')\n",
    "    print(model.model.evaluate(data_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 12:27:46.989800: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-01-19 12:27:47.082496: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-19 12:27:47.619333: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/780 [=======>......................] - ETA: 21s - loss: 0.7055 - binary_accuracy: 0.4980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/tan/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - 32s 39ms/step - loss: 0.7051 - binary_accuracy: 0.4996\n",
      "[0.7051015496253967, 0.4995793104171753]\n"
     ]
    }
   ],
   "source": [
    "model.model = keras.models.load_model('model.h5')\n",
    "print(model.model.evaluate(data_flow))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
