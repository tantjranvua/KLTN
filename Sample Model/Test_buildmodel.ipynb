{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab6e5f2-bf07-4aff-8b84-5d86bbe61bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 13:45:42.583997: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-03 13:45:42.584023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-03 13:45:42.585180: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-03 13:45:42.591126: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-03 13:45:43.261866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa243844-feee-4787-8c9f-74ff04b189ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 13:45:43.996864: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 13:45:44.064368: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 13:45:44.064544: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 13:45:44.066212: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 13:45:44.066373: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 13:45:44.066526: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 13:45:44.150657: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 13:45:44.150837: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 13:45:44.150939: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-01-03 13:45:44.150991: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 13:45:44.151094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6430 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "class buildmodel():\n",
    "    def residual_block(self, inputs, filter,block_num,reduce=False):\n",
    "        res = inputs\n",
    "        x = keras.layers.Conv2D(filter,(3, 3), strides = 2 if reduce else 1,activation = 'relu', padding = 'same',name = f'conv_{block_num}_1')(res)\n",
    "        x = keras.layers.Conv2D(filter,(3, 3), activation = 'relu', padding = 'same',name = f'conv_{block_num}_2')(x)\n",
    "        if reduce == True:\n",
    "            res = keras.layers.Conv2D(filter,(1, 1), strides = 2, activation = 'relu', padding = 'same',name = f'conv_skip_{block_num-1}')(res)\n",
    "        res = keras.layers.Add(name = f'end_res_bock_{2*block_num-1}')([x,res])\n",
    "        x = keras.layers.Conv2D(filter,(3, 3), activation = 'relu', padding = 'same',name = f'conv_{block_num}_3')(res)\n",
    "        x = keras.layers.Conv2D(filter,(3, 3), activation = 'relu', padding = 'same',name = f'conv_{block_num}_4')(x)\n",
    "        res = keras.layers.Add(name = f'end_res_bock_{2*block_num}')([x,res])\n",
    "        return res\n",
    "    \n",
    "    def resnet18(self, num_outputs=1):\n",
    "        # Inputs\n",
    "        inputs = keras.layers.Input(shape=(224,224,3),name = 'input')\n",
    "        x = keras.layers.Conv2D(64,(7, 7),strides = 2, activation = 'relu', padding = 'same',name = 'conv2D')(inputs)\n",
    "        x = keras.layers.MaxPooling2D(pool_size = (3,3),strides = 2 ,padding = 'same',name = 'max_pool_1')(x)\n",
    "        # Block 1\n",
    "        x = self.residual_block(x,64,1,False)\n",
    "        # Block 2\n",
    "        x = self.residual_block(x,128,2,True)\n",
    "        # Block 3\n",
    "        x = self.residual_block(x,256,3,True)\n",
    "        # Block 4\n",
    "        x = self.residual_block(x,512,4,True)\n",
    "        # Ouputs\n",
    "        x = keras.layers.GlobalAveragePooling2D(name = 'global_pooling')(x)\n",
    "        if num_outputs==1:\n",
    "            outputs = keras.layers.Dense(num_outputs,activation = 'sigmoid',name = 'outputs_sigmoid')(x)\n",
    "        else:\n",
    "            outputs = keras.layers.Dense(num_outputs,activation = 'softmax',name = 'outputs_softmax')(x)\n",
    "        model = keras.Model(inputs = inputs, outputs = outputs, name = 'resnet18')\n",
    "        return model\n",
    "    def __init__(self, optimizer=keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy() , metrics=keras.metrics.BinaryAccuracy()):\n",
    "        self.model = self.resnet18()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "    def compile(self, optimizer=keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy() , metrics=keras.metrics.BinaryAccuracy()):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.model.compile(optimizer = self.optimizer, loss = self.loss, metrics = self.metrics)\n",
    "    @tf.function\n",
    "    def train_step(self,x,y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(x, training=True)\n",
    "            loss_value = self.loss(y, logits)\n",
    "        grads = tape.gradient(loss_value, self.model.trainable_weights)\n",
    "        self.metrics.update_state(y, logits)\n",
    "        return loss_value,grads\n",
    "    @tf.function\n",
    "    def optimize_model(self, grads):\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbcd190-ea53-41cc-945f-f2e30ee0ba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24959 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "train_gen = img_gen.flow_from_directory('./data',class_mode=\"binary\",target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17881826-c75e-437f-bf45-e125f4db6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_build = buildmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab17ba0-6dcf-4214-a713-0311539445f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.7284\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6837\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6953\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6938\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.5223\n",
      "Training time over epoch: 50.33810234069824\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.6891\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6707\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6482\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.7024\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.5564\n",
      "Training time over epoch: 44.74661326408386\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.6535\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.7055\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6653\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6611\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.5754\n",
      "Training time over epoch: 44.52057409286499\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.6702\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6428\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6897\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6636\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.5904\n",
      "Training time over epoch: 44.76527142524719\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.7099\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.6783\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.5621\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.7888\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.6221\n",
      "Training time over epoch: 44.59461236000061\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.6749\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.5801\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.5381\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.5711\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.6635\n",
      "Training time over epoch: 44.57601523399353\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.5842\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.5636\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.5918\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.3997\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.7152\n",
      "Training time over epoch: 44.47719383239746\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.6001\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.5455\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.4731\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.5193\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.7527\n",
      "Training time over epoch: 44.406819105148315\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.5662\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.3783\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.3138\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.4488\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.7835\n",
      "Training time over epoch: 44.2554407119751\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.2867\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.3893\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.4520\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.5183\n",
      "Seen so far: 19232 samples\n",
      "Training acc over epoch: 0.8116\n",
      "Training time over epoch: 44.41608762741089\n",
      "Total time: 451.10418224334717\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = 10\n",
    "start_train_time = time.time() \n",
    "for epoch in range(epochs):\n",
    "    start_epoch = time.time()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_gen):\n",
    "        y_batch_train = y_batch_train.reshape(-1,1)\n",
    "        # print(step, x_batch_train.shape, y_batch_train.shape)\n",
    "        loss_value,grads = model_build.train_step(x_batch_train,y_batch_train)\n",
    "        model_build.optimize_model(grads)\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n",
    "        if step==len(train_gen)-1:\n",
    "            break\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = model_build.metrics.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "    print(\"Training time over epoch:\", time.time()-start_epoch)\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    model_build.metrics.reset_states()\n",
    "print(\"Total time:\", time.time()-start_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f439cb-d532-4648-a54e-2835ef31d752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 13:45:53.856029: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-01-03 13:45:53.948390: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-03 13:45:54.482461: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-03 13:45:55.351707: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f1d45446060 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-03 13:45:55.351726: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2070, Compute Capability 7.5\n",
      "2024-01-03 13:45:55.356597: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1704264355.435250  753146 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-01-03 13:45:56.465319: W external/local_tsl/tsl/framework/bfc_allocator.cc:366] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/780 [===========>..................] - ETA: 26s - loss: 0.7771 - binary_accuracy: 0.5313"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/tan/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - 55s 60ms/step - loss: 0.7208 - binary_accuracy: 0.5462\n",
      "Epoch 2/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.6737 - binary_accuracy: 0.5785\n",
      "Epoch 3/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.6586 - binary_accuracy: 0.6073\n",
      "Epoch 4/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.6153 - binary_accuracy: 0.6613\n",
      "Epoch 5/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.5775 - binary_accuracy: 0.6990\n",
      "Epoch 6/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.5593 - binary_accuracy: 0.7121\n",
      "Epoch 7/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.5156 - binary_accuracy: 0.7415\n",
      "Epoch 8/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.4691 - binary_accuracy: 0.7752\n",
      "Epoch 9/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.4070 - binary_accuracy: 0.8148\n",
      "Epoch 10/10\n",
      "780/780 [==============================] - 45s 58ms/step - loss: 0.3237 - binary_accuracy: 0.8554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f1dc469e590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_build.compile()\n",
    "model_build.model.fit(train_gen, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fbf16-5f1d-4a48-bd94-bdd4a538624d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
